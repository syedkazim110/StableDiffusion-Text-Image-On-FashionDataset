# -*- coding: utf-8 -*-
"""Stable_Diffusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14B4p5cFXkIITySTEK47ZE1RkPpBDOOLd
"""

!pip install --upgrade huggingface_hub

from huggingface_hub import notebook_login
notebook_login()

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/huggingface/diffusers.git
# %cd diffusers
!pip -q install -e .
# %cd ..

from datasets import load_dataset
dataset = load_dataset("tilak1114/deepfashion")
print(dataset)
sample = dataset['train'][1]
print('Caption example:', sample.get('caption') or sample.get('text') )
import matplotlib.pyplot as plt
plt.imshow(sample['image'])
plt.axis('off')
plt.show()

MODEL_NAME = "runwayml/stable-diffusion-v1-5"
OUTPUT_DIR = "/content/lora-fashion-output"
DATASET_NAME = "tilak1114/deepfashion"
IMAGE_COLUMN = "image"
CAPTION_COLUMN = "caption"

!pip install accelerate transformers datasets safetensors ftfy

!accelerate launch diffusers/examples/text_to_image/train_text_to_image_lora.py \
  --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \
  --dataset_name="tilak1114/deepfashion" \
  --caption_column="caption" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --max_train_steps=3000 \
  --learning_rate=1e-4 \
  --output_dir="lora-sd-output"

from diffusers import StableDiffusionPipeline
import torch

# Load base Stable Diffusion model
base_model = "runwayml/stable-diffusion-v1-5"  # Or whichever base you used for training
pipe = StableDiffusionPipeline.from_pretrained(
    base_model,
    torch_dtype=torch.float32
)

# Load LoRA weights
pipe.load_lora_weights("/content/lora-sd-output", weight_name="pytorch_lora_weights.safetensors")

# Your prompt (can be one of your captions)
prompt = "A high tall model wearing fashionable evening gown, studio lighting"



# Generate image
image = pipe(prompt).images[0]

# Save image
image.save("lora_output.png")

from transformers import CLIPProcessor, CLIPModel
import torch
from PIL import Image

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

image = Image.open("/content/lora_output.png")
text = ["a photo of a dress", "a photo of a sweat shirt"]

inputs = processor(text=text, images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)

# Similarity score
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)
print(probs)

from datasets import load_dataset
dataset = load_dataset("tilak1114/deepfashion")

from datasets import load_dataset
from diffusers import StableDiffusionPipeline
from transformers import CLIPProcessor, CLIPModel
import torch
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# -------- SETTINGS --------
LORA_PATH = "/content/lora-sd-output"
BASE_MODEL = "runwayml/stable-diffusion-v1-5"
NUM_SAMPLES = 3  # Number of samples to evaluate
DEVICE = "cpu"
# --------------------------

# Load dataset (train split only)
# dataset = load_dataset("tilak1114/deepfashion")
train_data = dataset["train"]

# Load Stable Diffusion with LoRA weights
pipe = StableDiffusionPipeline.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float32
).to(DEVICE)
pipe.load_lora_weights(LORA_PATH, weight_name="pytorch_lora_weights.safetensors")

# Load CLIP model for evaluation
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(DEVICE)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Function: Generate image & compute similarity
def evaluate_sample(caption):
    # Generate image
    image = pipe(caption).images[0]

    # Process for CLIP
    inputs = clip_processor(
        text=[caption],
        images=[image],
        return_tensors="pt",
        padding=True
    ).to(DEVICE)

    # Get similarity score
    with torch.no_grad():
        outputs = clip_model(**inputs)
        logits_per_image = outputs.logits_per_image
        score = logits_per_image.softmax(dim=1)[0][0].item()
    return score, image

# Run evaluation
scores = []
for i in range(min(NUM_SAMPLES, len(train_data))):
    sample = train_data[i]
    caption = sample.get("caption") or sample.get("text")
    score, image = evaluate_sample(caption)
    scores.append(score)

    # Display image with score
    plt.imshow(image)
    plt.axis("off")
    plt.title(f"Score: {score:.4f}")
    plt.show()

print(f"\nAverage CLIP Similarity: {np.mean(scores):.4f}")