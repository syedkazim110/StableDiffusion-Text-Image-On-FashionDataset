# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A2U4AqgdULTXz61EEmlwoYkJBk8vtmEu
"""

import os
os.environ["WANDB_MODE"] = "disabled"
os.environ["WANDB_DISABLED"] = "true"

pip install transformers datasets tensorflow accelerate

from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch

dataset = load_dataset("imdb")
dataset["train"] = dataset["train"].shuffle(seed=42).select(range(1000))
dataset["test"] = dataset["test"].shuffle(seed=42).select(range(200))

dataset['train']

dataset["train"][0]

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)
# Converts each string → token IDs.

# Adds [CLS] at start and [SEP] at end (BERT’s format).

# padding=True → makes all sequences in the batch the same length (by adding [PAD] tokens).

# truncation=True → shortens texts that are too long for BERT’s max sequence length (usually 512 tokens).

encoded_dataset = dataset.map(tokenize, batched=True)

# 3. Set format for PyTorch
encoded_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# 4. Load model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    weight_decay=0.01
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    tokenizer=tokenizer,
)

trainer.train()

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

dataset = load_dataset("imdb")
dataset["train"] = dataset["train"].shuffle(seed=42).select(range(15000))
dataset["test"]  = dataset["test"].shuffle(seed=42).select(range(3000))

dataset_split = dataset["train"].train_test_split(test_size=0.1, seed=42)
train_dataset = dataset_split["train"]
eval_dataset  = dataset_split["test"]

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)
# Converts each string → token IDs.

# Adds [CLS] at start and [SEP] at end (BERT’s format).

# padding=True → makes all sequences in the batch the same length (by adding [PAD] tokens).

# truncation=True → shortens texts that are too long for BERT’s max sequence length (usually 512 tokens).

from datasets import DatasetDict

dataset_split = DatasetDict({
    "train": dataset_split["train"],
    "eval": dataset_split["test"],
    "test": dataset["test"]  # keep the test set
})

encoded_dataset = dataset_split.map(tokenize, batched=True)

# 3. Set format for PyTorch
encoded_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# 4. Load model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

from transformers import EarlyStoppingCallback

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=6,  # we may stop early
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average="binary")
    acc = accuracy_score(p.label_ids, preds)
    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

!pip install transformers datasets evaluate -q

from datasets import load_dataset, DatasetDict
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import evaluate
import numpy as np

# 1. Load IMDB dataset
dataset = load_dataset("imdb")

# 2. Use more data for higher accuracy (not too tiny subsets)
dataset["train"] = dataset["train"].shuffle(seed=42).select(range(20000))  # 20k train
dataset["test"]  = dataset["test"].shuffle(seed=42).select(range(5000))    # 5k test

# 3. Split train into train+eval
dataset_split = dataset["train"].train_test_split(test_size=0.1, seed=42)

# 4. Create a DatasetDict with train, eval, and test
dataset_dict = DatasetDict({
    "train": dataset_split["train"],
    "eval": dataset_split["test"],
    "test": dataset["test"]
})

# 5. Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)

encoded_dataset = dataset_dict.map(tokenize, batched=True)

# 6. Rename 'label' to 'labels' for Trainer compatibility
encoded_dataset = encoded_dataset.rename_column("label", "labels")
encoded_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# 7. Load model
model = BertForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)

# 8. Metrics
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"]
    f1 = f1_metric.compute(predictions=predictions, references=labels)["f1"]
    return {"accuracy": acc, "f1": f1}

# 9. Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",          # for your version (check if it's eval_strategy or evaluation_strategy)
    save_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    report_to=[]  # Disable wandb
)

# 10. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["eval"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# 11. Train
trainer.train()

# 12. Final evaluation
results = trainer.evaluate(encoded_dataset["test"])
print(results)